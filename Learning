I implemented this model using spaCy, an NLP library in Python. 
To load the "English" language module, I used "spacy.load("en_core_web_sm"), but you could also use "spacy.load("en")" for simplicity's sake. 
Data would be stored in the form of documents, which the the base of your application! You will need to employ this model on the data. nlp(data) would activate a data object with the module on it.
A token is a unit of text in the document, which consists of either words or punctuations. Although it is originally split using spaces, any contractions like "don't" would be split into "do" and "n't."
Lemma is the base of any word (walk is the base of walking). Stopwords are words that are integral for sentence formation, but do not have any important information (is, and, are would be some examples).
token.lemma_ returns if a word (token) is in its base form, and token.is_stop returns if a token is a stop word or not. 
Although lemmatizing and dropping stopwords help the model to focus on important words, it may also impact it's performance. This should be considered as a hyperparameter in the optimization process.
You must create a "Matcher" to match individual tokens. It is a lot easier to use PhraseMatcher to match a list of words. 

Text classificaiton is a common task in Natural Language Processing. 
Machine learning models cannot learn from strings or texts; you need to convert all of them into some numeric data. 
However, all these words can be expressed in the form of word vectors with the occuerence frequencies. A "bag of words" representation is a set of vectors with the occurences of each word.
Term Frequency-Inverse Document Frequency is another way to do this, but I have not explored it.
Pipe classes help to process and transform tokens. To process the sentiment, you will need to add a pipe. 
You can create a text-categorization pipe with a configuration - in this case, I did it with a "bow architecture." 
nlp.create_pipe("textcat", config{"exclusive_classes":True, "architexture":"bow"}) lets you create a new pipe.
You can add labels to your text classifier.
Use the add_pipe() function to add the text categorizer to your NLP model.
To train a model, you need to split it into a training dataset and a testing dataset. You must not test on the same dataset, as you will not be able to judge its accuracy.

Labels in the data must be converted to a form the model requires. 
In the model I created, I set the executive classes to be true. So, the "spam" will be assigned a dictionary value of False, just like the "ham" will be assigned a value of True.
This would form the train labels for the model. You should combine the train data and the labels into a single list to facilitate training the model.
I used an optimizer to get my results faster. It is advised to train models in smaller batches, so you can use spacy's minibatch feature.
The batch generator (minibatch) allows you to define sizes of your batches.
